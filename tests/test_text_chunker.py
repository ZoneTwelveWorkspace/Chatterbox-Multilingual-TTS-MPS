# Chatterbox-Multilingual-TTS/tests/test_text_chunker.py
"""
Comprehensive test suite for the TextChunker module.

This module contains unit tests and integration tests to ensure the text chunking
functionality works correctly across all supported languages and edge cases.
"""

import sys
import unittest
from pathlib import Path

# Add src to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from chatterbox.text_chunker import (
    Chunk,
    TextChunker,
    chunk_text_with_info,
    smart_chunk_text,
)


class TestTextChunker(unittest.TestCase):
    """Test cases for the TextChunker class."""

    def setUp(self):
        """Set up test fixtures before each test method."""
        self.chunker = TextChunker(max_chars=300)

    def test_basic_chunking_english(self):
        """Test basic chunking with English text."""
        text = "This is a short text. This should stay as one chunk."
        chunks = self.chunker.chunk_text(text, "en")

        self.assertEqual(len(chunks), 1)
        self.assertEqual(chunks[0].text, text)
        self.assertEqual(chunks[0].char_count, len(text))
        self.assertTrue(chunks[0].is_final)

    def test_long_text_chunking_english(self):
        """Test chunking of long English text."""
        # Create text longer than 300 characters
        long_text = (
            "This is a very long text that definitely exceeds the 300 character limit. "
            "It contains multiple sentences with proper punctuation like periods, commas, and semicolons. "
            "The system should intelligently split this text at natural break points such as sentence endings. "
            "This ensures that each chunk maintains natural speech flow when converted to audio. "
            "The algorithm looks for periods, question marks, exclamation points, and other suitable punctuation marks. "
            "When no natural breaks are found, it falls back to word boundaries to maintain readability. "
            "This comprehensive approach ensures high-quality text-to-speech conversion across multiple languages."
        )

        chunks = self.chunker.chunk_text(long_text, "en")

        # Verify all chunks are within limits
        for chunk in chunks:
            self.assertLessEqual(chunk.char_count, 300)
            self.assertGreater(len(chunk.text.strip()), 0)

        # Verify no chunks are empty
        self.assertTrue(all(chunk.text.strip() for chunk in chunks))

        # Verify reconstruction is accurate
        reconstructed = " ".join(chunk.text for chunk in chunks)
        self.assertIn("This is a very long text that definitely exceeds", reconstructed)

    def test_chinese_text_chunking(self):
        """Test chunking with Chinese text and punctuation."""
        chinese_text = (
            "è¿™æ˜¯ä¸€ä¸ªå¾ˆé•¿çš„ä¸­æ–‡æ–‡æœ¬ï¼Œéœ€è¦åˆ†æˆå°å—è¿›è¡Œå¤„ç†ã€‚ç³»ç»Ÿä¼šè‡ªåŠ¨å¯»æ‰¾åˆé€‚çš„ä¸­æ–‡æ ‡ç‚¹ç¬¦å·ä½œä¸ºæ–­ç‚¹ã€‚"
            "æ¯ä¸ªåˆ†å—éƒ½ä¼šä¼˜åŒ–åˆ°300å­—ç¬¦ä»¥å†…ï¼ŒåŒæ—¶ä¿æŒè‡ªç„¶çš„è¯­éŸ³æµç•…åº¦ã€‚è¿™ç§æ–¹æ³•ç¡®ä¿æ›´å¥½çš„éŸ³é¢‘è´¨é‡å’Œæ›´è‡ªç„¶çš„è¯­éŸ³åˆæˆæ•ˆæœã€‚"
            "ä¸­æ–‡æ–‡æœ¬å¤„ç†éœ€è¦ç‰¹åˆ«æ³¨æ„ä¸­æ–‡å­—ç¬¦çš„ç‰¹æ€§å’Œä¸­æ–‡æ ‡ç‚¹ç¬¦å·çš„ä½¿ç”¨ã€‚"
            "éšç€äººå·¥æ™ºèƒ½æŠ€æœ¯çš„ä¸æ–­å‘å±•ï¼Œå¤šè¯­è¨€æ–‡æœ¬å¤„ç†æˆä¸ºäº†ä¸€ä¸ªé‡è¦çš„ç ”ç©¶æ–¹å‘ã€‚"
            "è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯åœ¨è·¨è¯­è¨€åº”ç”¨ä¸­é¢ä¸´ç€è®¸å¤šæŒ‘æˆ˜ï¼ŒåŒ…æ‹¬è¯­æ³•ç»“æ„çš„å·®å¼‚ã€è¯æ±‡çš„å¤šæ ·æ€§ä»¥åŠè¯­ä¹‰ç†è§£çš„å¤æ‚æ€§ã€‚"
            "å› æ­¤ï¼Œå¼€å‘æœ‰æ•ˆçš„æ–‡æœ¬åˆ†å—ç®—æ³•å¯¹äºæé«˜å¤„ç†æ•ˆç‡å’Œè´¨é‡å…·æœ‰é‡è¦æ„ä¹‰ã€‚"
            "é€šè¿‡æ™ºèƒ½åŒ–çš„åˆ†å—ç­–ç•¥ï¼Œæˆ‘ä»¬å¯ä»¥æ›´å¥½åœ°å¤„ç†å„ç§è¯­è¨€çš„é•¿æ–‡æœ¬ï¼Œç¡®ä¿æ¯ä¸ªåˆ†å—éƒ½èƒ½åœ¨æŒ‡å®šçš„å­—ç¬¦é™åˆ¶å†…è¿›è¡Œæœ‰æ•ˆçš„å¤„ç†ã€‚"
            "æœºå™¨å­¦ä¹ ç®—æ³•åœ¨æ–‡æœ¬åˆ†æä¸­å‘æŒ¥ç€è¶Šæ¥è¶Šé‡è¦çš„ä½œç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤§è§„æ¨¡å¤šè¯­è¨€æ•°æ®é›†æ—¶ã€‚"
            "æ·±åº¦å­¦ä¹ æ¨¡å‹çš„å…´èµ·ä¸ºè‡ªç„¶è¯­è¨€å¤„ç†å¸¦æ¥äº†é©å‘½æ€§çš„å˜åŒ–ï¼Œä½¿å¾—æœºå™¨èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€ã€‚"
            "ç„¶è€Œï¼Œä¸åŒè¯­è¨€ä¹‹é—´çš„è¯­æ³•å·®å¼‚ã€è¯æ±‡ç»“æ„å’Œæ–‡åŒ–èƒŒæ™¯ä»ç„¶æ˜¯å¯¹è¿™äº›æŠ€æœ¯çš„é‡å¤§æŒ‘æˆ˜ã€‚"
            "å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦æ›´åŠ æ™ºèƒ½å’Œé€‚åº”æ€§çš„æ–¹æ³•æ¥å¤„ç†è¿™äº›å¤æ‚æ€§ï¼Œç¡®ä¿è·¨è¯­è¨€åº”ç”¨çš„æœ‰æ•ˆæ€§å’Œå‡†ç¡®æ€§ã€‚"
            "æ–‡æœ¬åˆ†å—ä½œä¸ºæ–‡æœ¬é¢„å¤„ç†çš„é‡è¦æ­¥éª¤ï¼Œç›´æ¥å½±å“åç»­å¤„ç†çš„è´¨é‡å’Œæ•ˆç‡ã€‚"
            "ç‰¹åˆ«æ˜¯åœ¨è¯­éŸ³åˆæˆåº”ç”¨ä¸­ï¼Œåˆé€‚çš„åˆ†å—ç­–ç•¥å¯ä»¥æ˜¾è‘—æé«˜ç”ŸæˆéŸ³é¢‘çš„è‡ªç„¶åº¦å’Œæµç•…æ€§ã€‚"
            "é€šè¿‡åˆç†æ§åˆ¶æ¯ä¸ªæ–‡æœ¬ç‰‡æ®µçš„é•¿åº¦ï¼Œæˆ‘ä»¬å¯ä»¥æ›´å¥½åœ°å¹³è¡¡å¤„ç†é€Ÿåº¦å’Œè¾“å‡ºè´¨é‡ä¹‹é—´çš„å…³ç³»ã€‚"
        )

        chunks = self.chunker.chunk_text(chinese_text, "zh")

        # Verify all chunks are within limits
        for chunk in chunks:
            self.assertLessEqual(chunk.char_count, 300)
            self.assertGreater(len(chunk.text.strip()), 0)

        # Should have multiple chunks for long text
        self.assertGreater(len(chunks), 1)

        # Verify chunks contain Chinese characters
        for chunk in chunks:
            self.assertTrue(any(ord(char) > 127 for char in chunk.text))

    def test_japanese_text_chunking(self):
        """Test chunking with Japanese text and punctuation."""
        japanese_text = (
            "ã“ã‚Œã¯éå¸¸ã«é•·ã„æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã§ã™ã€‚è‡ªå‹•çš„ã«é©åˆ‡ãªåŒºåˆ‡ã‚Šç‚¹ã§åˆ†å‰²ã•ã‚Œã¾ã™ã€‚"
            "å„ãƒãƒ£ãƒ³ã‚¯ã¯300æ–‡å­—ä»¥å†…ã«æœ€é©åŒ–ã•ã‚Œã€è‡ªç„¶ãªéŸ³å£°ãƒ•ãƒ­ãƒ¼ã‚’ç¶­æŒã—ã¾ã™ã€‚"
            "ã“ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã«ã‚ˆã‚Šã€éŸ³å£°åˆæˆã®å“è³ªãŒå‘ä¸Šã—ã¾ã™ã€‚"
        )

        chunks = self.chunker.chunk_text(japanese_text, "ja")

        # Verify all chunks are within limits
        for chunk in chunks:
            self.assertLessEqual(chunk.char_count, 300)

        # Should handle Japanese punctuation correctly
        self.assertTrue(any("ã€‚" in chunk.text for chunk in chunks))

    def test_arabic_text_chunking(self):
        """Test chunking with Arabic text."""
        arabic_text = (
            "Ù‡Ø°Ø§ Ù†Øµ Ø¹Ø±Ø¨ÙŠ Ø·ÙˆÙŠÙ„ Ø¬Ø¯Ø§Ù‹ ÙŠØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø³ÙŠÙ… Ø¥Ù„Ù‰ Ù‚Ø·Ø¹ Ø£ØµØºØ±. "
            "Ø³ÙŠÙ‚ÙˆÙ… Ø§Ù„Ù†Ø¸Ø§Ù… Ø¨Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù†Ù‚Ø§Ø· Ø§Ù„ØªÙˆÙ‚Ù Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù…Ø«Ù„ Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªØ±Ù‚ÙŠÙ… Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©."
            " ÙƒÙ„ Ù‚Ø·Ø¹Ø© Ø³ØªÙƒÙˆÙ† Ù…Ø­Ø³Ù†Ø© Ù„Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ 300 Ø­Ø±Ù Ù…Ø¹ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„ØªØ¯ÙÙ‚ Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠ Ù„Ù„ÙƒÙ„Ø§Ù…."
        )

        chunks = self.chunker.chunk_text(arabic_text, "ar")

        # Verify all chunks are within limits
        for chunk in chunks:
            self.assertLessEqual(chunk.char_count, 300)

        # Should contain Arabic text
        self.assertTrue(any(ord(char) > 127 for char in chunks[0].text))

    def test_mixed_punctuation_chunking(self):
        """Test chunking with mixed punctuation marks."""
        text = (
            "First sentence; second sentence: third sentence, fourth sentence! "
            "Fifth sentence? Sixth sentenceâ€”seventh sentence. Eighth sentence; "
            "ninth sentence: tenth sentence, eleventh sentence! Twelfth sentence? "
            "Thirteenth sentenceâ€”fourteenth sentence. This long text should definitely "
            "be split into multiple chunks because it exceeds the 300 character limit "
            "and contains multiple punctuation marks that can serve as natural break points "
            "for the chunking algorithm to work effectively."
        )

        chunks = self.chunker.chunk_text(text, "en")

        # Should split at various punctuation marks
        self.assertGreater(len(chunks), 1)

        # Each chunk should end with punctuation or be the final chunk
        for i, chunk in enumerate(chunks):
            if not chunk.is_final:
                self.assertTrue(
                    any(
                        chunk.text.rstrip().endswith(punct)
                        for punct in [";", ":", ",", "â€”", "!", "?", "."]
                    )
                )

    def test_edge_case_empty_text(self):
        """Test chunking with empty text."""
        chunks = self.chunker.chunk_text("", "en")

        self.assertEqual(len(chunks), 1)
        self.assertEqual(chunks[0].text, "")
        self.assertEqual(chunks[0].char_count, 0)
        self.assertTrue(chunks[0].is_final)

    def test_edge_case_single_long_word(self):
        """Test chunking with a single word longer than 300 characters."""
        long_word = (
            "supercalifragilisticexpialidocious" * 30
        )  # Much longer "word" to ensure splitting

        chunks = self.chunker.chunk_text(long_word, "en")

        # Should split the long word into multiple chunks
        self.assertGreater(len(chunks), 1)

        # All chunks should be within limit
        for chunk in chunks:
            self.assertLessEqual(chunk.char_count, 300)

    def test_word_boundary_chunking(self):
        """Test chunking when no punctuation is available."""
        text = " ".join([f"word{i}" for i in range(1, 50)])  # Many words

        chunks = self.chunker.chunk_text(text, "en")

        # Should split at word boundaries
        self.assertGreater(len(chunks), 1)

        # Verify no chunks exceed the limit
        for chunk in chunks:
            self.assertLessEqual(chunk.char_count, 300)

    def test_unicode_handling(self):
        """Test handling of various Unicode characters."""
        unicode_text = "Hello ğŸŒ Ù…Ø±Ø­Ø¨Ø§ ã“ã‚“ã«ã¡ã¯ ĞŸÑ€Ğ¸Ğ²ĞµÑ‚ ğŸµ"

        chunks = self.chunker.chunk_text(unicode_text, "en")

        self.assertEqual(len(chunks), 1)
        self.assertIn("ğŸŒ", chunks[0].text)
        self.assertIn("Ù…Ø±Ø­Ø¨Ø§", chunks[0].text)
        self.assertIn("ã“ã‚“ã«ã¡ã¯", chunks[0].text)

    def test_custom_max_chars(self):
        """Test chunking with custom maximum character limit."""
        chunker = TextChunker(max_chars=50)
        long_text = "This is a text that should be split into multiple chunks of 50 characters or less."

        chunks = chunker.chunk_text(long_text, "en")

        for chunk in chunks:
            self.assertLessEqual(chunk.char_count, 50)

    def test_chunk_metadata(self):
        """Test chunking with detailed metadata."""
        text = "Short text."
        metadata = self.chunker.chunk_with_metadata(text, "en")

        self.assertEqual(metadata["original_length"], len(text))
        self.assertEqual(metadata["chunk_count"], 1)
        self.assertEqual(metadata["total_chars_in_chunks"], len(text))
        self.assertEqual(metadata["language"], "en")
        self.assertEqual(metadata["max_chars"], 300)
        self.assertEqual(metadata["compression_ratio"], 1.0)

    def test_chunk_validation(self):
        """Test chunk validation functionality."""
        # Valid chunks
        valid_chunks = [
            Chunk("Short text", 0, 10, True),
            Chunk("Another chunk", 1, 13, False),
        ]
        self.assertTrue(self.chunker.validate_chunks(valid_chunks))

        # Invalid chunks (exceeds limit)
        invalid_chunks = [Chunk("a" * 400, 0, 400, True)]
        self.assertFalse(self.chunker.validate_chunks(invalid_chunks))

    def test_utility_functions(self):
        """Test utility functions."""
        text = "This is a test. This is another test."

        # Test smart_chunk_text
        chunks = smart_chunk_text(text, "en", 50)
        self.assertIsInstance(chunks, list)
        self.assertIsInstance(chunks[0], str)

        # Test chunk_text_with_info
        info = chunk_text_with_info(text, "en", 50)
        self.assertIn("chunks", info)
        self.assertIn("original_length", info)
        self.assertIn("chunk_count", info)
        self.assertIsInstance(info["chunks"], list)

    def test_multilingual_punctuation_support(self):
        """Test punctuation support across different languages."""
        test_cases = {
            "en": "Hello! How are you? I'm fine.",
            "zh": "ä½ å¥½ï¼ä½ å¥½å—ï¼Ÿæˆ‘å¾ˆå¥½ã€‚",
            "ja": "ã“ã‚“ã«ã¡ã¯ï¼å…ƒæ°—ã§ã™ã‹ï¼Ÿã¯ã„ã€å…ƒæ°—ã§ã™ã€‚",
            "ar": "Ù…Ø±Ø­Ø¨Ø§! ÙƒÙŠÙ Ø­Ø§Ù„ÙƒØŸ Ø£Ù†Ø§ Ø¨Ø®ÙŠØ±.",
            "ru": "ĞŸÑ€Ğ¸Ğ²ĞµÑ‚! ĞšĞ°Ğº Ğ´ĞµĞ»Ğ°? Ğ£ Ğ¼ĞµĞ½Ñ Ğ²ÑĞµ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾.",
            "fr": "Bonjour! Comment allez-vous? Je vais bien.",
        }

        for lang, text in test_cases.items():
            chunks = self.chunker.chunk_text(text, lang)
            self.assertGreater(len(chunks), 0)
            for chunk in chunks:
                self.assertLessEqual(chunk.char_count, 300)

    def test_performance_with_very_long_text(self):
        """Test performance with very long text (1000+ characters)."""
        # Generate very long text
        sentences = [
            "This is sentence number {} of a very long text that will test the performance of the chunking algorithm. ",
            "Each sentence contains multiple clauses and should be processed efficiently by the text chunker. ",
            "The system should maintain good performance even with extensive text processing requirements. ",
            "Quality and speed are both important factors in this comprehensive testing scenario. ",
        ]

        long_text = "".join(
            sentences[(i - 1) % len(sentences)].format(i) for i in range(1, 20)
        )
        self.assertGreater(len(long_text), 1000)

        chunks = self.chunker.chunk_text(long_text, "en")

        # Performance check - should complete quickly
        self.assertGreater(len(chunks), 1)

        # Verify all chunks are valid
        self.assertTrue(self.chunker.validate_chunks(chunks))

        # Verify reconstruction maintains order
        reconstructed_chunks = [chunk.text for chunk in chunks]
        for i in range(len(reconstructed_chunks) - 1):
            # Each chunk (except last) should end with appropriate punctuation
            chunk_text = reconstructed_chunks[i]
            has_ending_punct = any(
                chunk_text.rstrip().endswith(punct)
                for punct in [".", "!", "?", ";", ":"]
            )
            self.assertTrue(
                has_ending_punct, f"Chunk {i} doesn't end with proper punctuation"
            )

    def test_consecutive_punctuation_handling(self):
        """Test handling of consecutive punctuation marks."""
        text = (
            "What?! Really??? Yes!!! Absolutely... This should definitely be long enough to require chunking. "
            * 5
        )

        chunks = self.chunker.chunk_text(text, "en")

        # Should handle consecutive punctuation correctly
        self.assertGreater(len(chunks), 0)

        # Verify no chunk has consecutive sentence endings that could be optimized
        for chunk in chunks:
            # Should not have multiple consecutive sentence endings
            sentence_endings = [".", "!", "?"]
            consecutive_endings = 0
            for char in reversed(chunk.text):
                if char in sentence_endings:
                    consecutive_endings += 1
                else:
                    break
            # Allow up to 3 consecutive endings to be more lenient
            self.assertLessEqual(consecutive_endings, 3)

    def test_whitespace_preservation(self):
        """Test that whitespace is handled correctly."""
        text = "  Leading and trailing spaces  should be handled.  \n\n  Multiple newlines too.  "

        chunks = self.chunker.chunk_text(text, "en")

        # Should preserve meaningful whitespace
        for chunk in chunks:
            # After stripping for display, original should be preserved
            original_preserved = any(chunk.text.strip() in text for chunk in chunks)
            self.assertTrue(original_preserved)


class TestChunkMetadata(unittest.TestCase):
    """Test cases for chunk metadata and indexing."""

    def setUp(self):
        self.chunker = TextChunker()

    def test_chunk_indexing(self):
        """Test that chunks have correct indices."""
        text = "First chunk. Second chunk! Third chunk?"

        chunks = self.chunker.chunk_text(text, "en")

        indices = [chunk.index for chunk in chunks]
        self.assertEqual(indices, list(range(len(chunks))))

    def test_final_chunk_marker(self):
        """Test that the final chunk is properly marked."""
        text = "Chunk one. Chunk two. Chunk three."

        chunks = self.chunker.chunk_text(text, "en")

        # Only the last chunk should be marked as final
        final_chunks = [chunk for chunk in chunks if chunk.is_final]
        self.assertEqual(len(final_chunks), 1)
        self.assertEqual(final_chunks[0].index, len(chunks) - 1)

    def test_char_count_accuracy(self):
        """Test that character counts are accurate."""
        text = "Hello world! This has 30 characters."

        chunks = self.chunker.chunk_text(text, "en")

        for chunk in chunks:
            self.assertEqual(chunk.char_count, len(chunk.text))


class TestLanguageSpecificFeatures(unittest.TestCase):
    """Test cases for language-specific features."""

    def setUp(self):
        self.chunker = TextChunker()

    def test_language_punctuation_mapping(self):
        """Test that different languages use correct punctuation."""
        test_cases = {
            "zh": ["ã€‚", "ï¼", "ï¼Ÿ"],
            "ja": ["ã€‚", "ï¼", "ï¼Ÿ"],
            "ar": [".", "!", "?", "Û”"],
            "hi": ["à¥¤", "!", "?"],
        }

        for lang, expected_punct in test_cases.items():
            sentence_endings, secondary_breaks = (
                self.chunker.get_punctuation_for_language(lang)
            )

            # Check that the language has its specific punctuation
            for punct in expected_punct:
                self.assertIn(punct, sentence_endings)

    def test_unknown_language_fallback(self):
        """Test that unknown languages fall back to English punctuation."""
        chunks = self.chunker.chunk_text("Test sentence.", "unknown_lang")

        self.assertEqual(len(chunks), 1)
        self.assertEqual(chunks[0].text, "Test sentence.")


if __name__ == "__main__":
    # Create test suite
    test_suite = unittest.TestSuite()

    # Add test cases
    test_classes = [TestTextChunker, TestChunkMetadata, TestLanguageSpecificFeatures]

    for test_class in test_classes:
        tests = unittest.TestLoader().loadTestsFromTestCase(test_class)
        test_suite.addTests(tests)

    # Run tests with verbose output
    runner = unittest.TextTestRunner(verbosity=2, stream=sys.stdout)
    result = runner.run(test_suite)

    # Print summary
    print(f"\n{'=' * 60}")
    print(f"Test Summary:")
    print(f"Tests run: {result.testsRun}")
    print(f"Failures: {len(result.failures)}")
    print(f"Errors: {len(result.errors)}")
    print(
        f"Success rate: {((result.testsRun - len(result.failures) - len(result.errors)) / result.testsRun * 100):.1f}%"
    )

    if result.failures:
        print(f"\nFailures:")
        for test, traceback in result.failures:
            print(f"- {test}: {traceback.split('AssertionError:')[-1].strip()}")

    if result.errors:
        print(f"\nErrors:")
        for test, traceback in result.errors:
            print(f"- {test}: {traceback.split('Exception:')[-1].strip()}")

    print(f"{'=' * 60}")
